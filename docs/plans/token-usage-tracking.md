# Token Usage Tracking Feature Plan

**Status:** Implemented
**Date:** 2025-01-20

## Overview

Implement a dedicated token usage tracking system, separated from audit logs, with a feature flag to enable/disable it.

---

## Research Findings

### Current State Analysis

**1. Core Usage Struct is Minimal** (`internal/core/types.go`)
```go
type Usage struct {
    PromptTokens     int `json:"prompt_tokens"`
    CompletionTokens int `json:"completion_tokens"`
    TotalTokens      int `json:"total_tokens"`
}
```
- Only 3 basic fields - no extended metrics (cached, reasoning, etc.)
- Extended usage data from providers is lost during conversion

**2. Streaming Usage NOT Currently Extracted**
- OpenAI streaming: Usage NOT available in chunks (need `stream_options.include_usage`)
- Anthropic streaming: Usage IS available in `message_start`/`message_delta` events BUT currently ignored
- Current `StreamLogWrapper` buffers last 8KB but doesn't extract extended usage

**3. Request ID Already Generated by Audit Log**
- `internal/auditlog/middleware.go` generates X-Request-ID if missing
- Stored in Echo context - can be reused (no new middleware needed)

**4. Audit Log Patterns to Reuse**
- Async buffered logger with channel-based queue
- Dual-trigger flush (batch threshold 100 + time interval 5s)
- Store interface with SQLite/PostgreSQL/MongoDB implementations
- Factory pattern: `New()` → `*Result{Logger, Storage}`

### Provider Token Types (Extended)

| Provider | Response ID Format | Standard Tokens | Extended Fields |
|----------|-------------------|-----------------|-----------------|
| **OpenAI** | `chatcmpl-xxx` | prompt_tokens, completion_tokens | `cached_tokens`, `reasoning_tokens` |
| **Anthropic** | `msg_xxx` | input_tokens, output_tokens | `cache_creation_input_tokens`, `cache_read_input_tokens` |
| **Gemini** | `chatcmpl-xxx` | prompt_tokens, completion_tokens | `cached_tokens`, `thought_tokens`, `tool_use_tokens` |
| **Groq** | `chatcmpl-xxx` | prompt_tokens, completion_tokens | (none significant) |
| **xAI** | `chatcmpl-xxx` | prompt_tokens, completion_tokens | `reasoning_tokens`, `cached_tokens`, `image_tokens` |

All providers return a unique response ID in the `id` field of their response.

---

## Proposed Architecture

### 1. UsageEntry Schema

```go
type UsageEntry struct {
    // Core identifiers
    ID          string    `json:"id"`           // Our UUID for this usage entry
    RequestID   string    `json:"request_id"`   // Our request UUID (links to audit log, from X-Request-ID)
    ProviderID  string    `json:"provider_id"`  // Provider's response ID (e.g., "chatcmpl-abc123", "msg_xyz")
    Timestamp   time.Time `json:"timestamp"`

    // Request context
    Model    string `json:"model"`
    Provider string `json:"provider"`
    Endpoint string `json:"endpoint"`  // /v1/chat/completions, /v1/responses

    // Standard token counts (always present, normalized)
    InputTokens  int `json:"input_tokens"`
    OutputTokens int `json:"output_tokens"`
    TotalTokens  int `json:"total_tokens"`

    // Provider-specific extended data (JSONB)
    RawData map[string]any `json:"raw_data,omitempty"`
}
```

**Why two IDs?**
- `request_id` - Same across retries, links to audit log, enables JOIN queries
- `provider_id` - Provider's unique response ID for debugging and support tickets

**RawData examples by provider:**
```json
// OpenAI (o-series with caching)
{"cached_tokens": 100, "reasoning_tokens": 50}

// Anthropic (with prompt caching)
{"cache_creation_input_tokens": 200, "cache_read_input_tokens": 150}

// Gemini (with thinking)
{"cached_tokens": 100, "thought_tokens": 75, "tool_use_tokens": 25}
```

### 2. Configuration

```go
// In config/config.go
type UsageConfig struct {
    Enabled       bool `mapstructure:"enabled"`        // Master toggle
    BufferSize    int  `mapstructure:"buffer_size"`    // Async buffer (default: 1000)
    FlushInterval int  `mapstructure:"flush_interval"` // Seconds (default: 5)
    RetentionDays int  `mapstructure:"retention_days"` // 0 = forever (default: 90)
}
// Note: Storage backend inherited from Logging.StorageType (shared DB)
```

**Environment variables:**
```bash
USAGE_ENABLED=true/false        # Feature flag
USAGE_BUFFER_SIZE=1000          # Async buffer capacity
USAGE_FLUSH_INTERVAL=5          # Flush interval in seconds
USAGE_RETENTION_DAYS=90         # Data retention (0 = forever)
```

### 3. Package Structure

```
internal/usage/
├── usage.go             # UsageEntry type
├── store.go             # UsageStore interface
├── store_sqlite.go      # SQLite implementation (chunked batches)
├── store_postgresql.go  # PostgreSQL implementation (adaptive transactions)
├── store_mongodb.go     # MongoDB implementation (TTL indexes)
├── logger.go            # Async buffered logger
├── extractor.go         # Extract usage from responses (per-provider logic)
└── factory.go           # New() factory function
```

### 4. Data Flow

```
┌─────────────────────────────────────────────────────────────────────────┐
│ Non-streaming:                                                           │
│   Handler → Provider.ChatCompletion() → ChatResponse                     │
│                                              ↓                           │
│                                    UsageExtractor.Extract(response, raw) │
│                                              ↓                           │
│                                    UsageLogger.Log(entry)                │
│                                              ↓                           │
│                                    [Async buffer → batch flush]          │
│                                              ↓                           │
│                                    UsageStore.WriteBatch()               │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ Streaming:                                                               │
│   Handler → Provider.StreamChatCompletion() → io.ReadCloser              │
│                                                    ↓                     │
│                                    UsageStreamWrapper (wraps stream)     │
│                                                    ↓                     │
│   [On Close()] → Extract usage from buffered final chunk                 │
│                                                    ↓                     │
│                                    UsageLogger.Log(entry)                │
└─────────────────────────────────────────────────────────────────────────┘
```

### 5. Database Schema

**SQL (SQLite/PostgreSQL):**
```sql
CREATE TABLE usage (
    id TEXT PRIMARY KEY,
    request_id TEXT NOT NULL,
    provider_id TEXT NOT NULL,        -- Provider's response ID (e.g., "chatcmpl-abc123")
    timestamp TIMESTAMP NOT NULL,
    model TEXT NOT NULL,
    provider TEXT NOT NULL,
    endpoint TEXT NOT NULL,
    input_tokens INTEGER NOT NULL DEFAULT 0,
    output_tokens INTEGER NOT NULL DEFAULT 0,
    total_tokens INTEGER NOT NULL DEFAULT 0,
    raw_data JSONB
);

CREATE INDEX idx_usage_timestamp ON usage(timestamp);
CREATE INDEX idx_usage_request_id ON usage(request_id);
CREATE INDEX idx_usage_provider_id ON usage(provider_id);
CREATE INDEX idx_usage_model ON usage(model);
CREATE INDEX idx_usage_provider ON usage(provider);
```

**MongoDB:**
```javascript
{
    _id: "uuid",
    request_id: "uuid",
    provider_id: "chatcmpl-abc123",   // Provider's response ID
    timestamp: ISODate,
    model: "gpt-4",
    provider: "openai",
    endpoint: "/v1/chat/completions",
    input_tokens: 100,
    output_tokens: 50,
    total_tokens: 150,
    raw_data: { cached_tokens: 20, reasoning_tokens: 10 }
}
// TTL index on timestamp for automatic cleanup
// Index on provider_id for provider-side correlation
```

---

## Files to Create/Modify

### New Files

| File | Description |
|------|-------------|
| `internal/usage/usage.go` | UsageEntry type definition |
| `internal/usage/store.go` | UsageStore interface |
| `internal/usage/store_sqlite.go` | SQLite store (follow auditlog pattern) |
| `internal/usage/store_postgresql.go` | PostgreSQL store |
| `internal/usage/store_mongodb.go` | MongoDB store with TTL |
| `internal/usage/logger.go` | Async buffered logger |
| `internal/usage/extractor.go` | Per-provider usage extraction |
| `internal/usage/stream_wrapper.go` | Streaming response wrapper |
| `internal/usage/factory.go` | Factory function |

### Modified Files

| File | Change |
|------|--------|
| `config/config.go` | Add `UsageConfig` struct + viper defaults + env vars |
| `.env.template` | Add USAGE_* environment variables |
| `internal/app/app.go` | Initialize usage logger in `New()`, close in shutdown |
| `internal/server/handlers.go` | Call usage extractor after responses |
| `internal/core/types.go` | Add `RawUsage map[string]any` to Usage struct |

---

## Implementation Phases

### Phase 1: Core Types and Configuration
1. Add `UsageConfig` to `config/config.go` (follow LogConfig pattern)
2. Add viper defaults and env var expansion
3. Update `.env.template`
4. Create `internal/usage/usage.go` with UsageEntry

### Phase 2: Storage Layer
1. Create `store.go` with UsageStore interface
2. Implement `store_sqlite.go` (copy auditlog pattern, chunked batches)
3. Implement `store_postgresql.go` (adaptive transactions)
4. Implement `store_mongodb.go` (TTL indexes)

### Phase 3: Logger and Extractor
1. Create `logger.go` (async buffered, channel-based)
2. Create `extractor.go` with per-provider extraction logic
3. Create `stream_wrapper.go` for streaming responses
4. Create `factory.go` to wire everything together

### Phase 4: Integration
1. Modify `core/types.go` to add RawUsage field
2. Initialize usage logger in `app/app.go`
3. Hook into handlers to extract and log usage
4. Handle streaming via wrapper

### Phase 5: Testing
1. Unit tests for extractor (each provider format)
2. Unit tests for logger (async behavior, shutdown)
3. Integration test (request → usage logged)
4. Feature flag test (disabled = no logging)

---

## Key Design Decisions

### Decision 1: Reuse Request ID from Audit Log
The audit log middleware already generates X-Request-ID. We'll retrieve it from context rather than creating a new middleware.
```go
requestID := c.Response().Header().Get("X-Request-ID")
```

### Decision 2: Add RawUsage to core.Usage
Extend the core Usage struct to preserve extended metrics:
```go
type Usage struct {
    PromptTokens     int            `json:"prompt_tokens"`
    CompletionTokens int            `json:"completion_tokens"`
    TotalTokens      int            `json:"total_tokens"`
    RawUsage         map[string]any `json:"raw_usage,omitempty"` // Extended provider-specific data
}
```
This requires updating each provider to populate RawUsage from their response.

### Decision 3: Separate Stream Wrapper (Not Reuse Audit Log's)
Create a dedicated `UsageStreamWrapper` rather than modifying audit log's `StreamLogWrapper`:
- Keeps concerns separated (audit log vs usage tracking)
- Can be enabled/disabled independently
- Simpler to test and maintain

### Decision 4: Shared Storage Backend
Usage tracking inherits storage config from `Logging.StorageType`:
- Same database, different table
- Enables JOIN queries on request_id
- Simpler deployment (one DB connection)

### Decision 5: Dual ID Tracking
Store both our `request_id` and the provider's `provider_id`:
- `request_id` - Our UUID, same across retries, links to audit log
- `provider_id` - Provider's response ID (e.g., `chatcmpl-abc123`, `msg_xyz`)
- Enables correlation with provider-side logs for debugging
- Useful for support tickets with providers

---

## Verification Plan

1. **Unit tests:**
   - Extractor correctly parses each provider's usage format
   - Logger async behavior and graceful shutdown
   - Store batch operations work correctly

2. **Integration test:**
   - Send request through system
   - Verify usage entry written to database
   - Verify request_id matches audit log

3. **Feature flag test:**
   - Set `USAGE_ENABLED=false`
   - Verify no usage entries created
   - Verify no performance impact (NoopLogger)

4. **Streaming test:**
   - Send streaming request
   - Verify usage extracted from final chunk
   - Verify entry logged correctly
